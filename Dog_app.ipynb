{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input/images/pp\"))\n",
    "data = \"../input/images/pp/\"\n",
    "\n",
    "\n",
    "#import datasets \n",
    "import numpy as np\n",
    "#use glob instead of writing code to scan the directory contents yourself\n",
    "from glob import glob\n",
    "\n",
    "#load filenames for human and dog images \n",
    "human_images = np.array(glob(\"../input/images/pp/lfw/*/*/*\"))\n",
    "dog_images = np.array(glob(\"../input/images/pp/dogImages/*/*/*\"))\n",
    "\n",
    "#print number of images in each dataset\n",
    "print('There are %d total human images.' % len(human_images))\n",
    "print('There are %d total dog images.' % len(dog_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  step 2 : Detect human\n",
    "\n",
    "\n",
    "\n",
    "In this section, we use OpenCV's implementation of Haar feature-based cascade classifiers to detect human faces in images.\n",
    "\n",
    "OpenCV provides many pre-trained face detectors, stored as XML files on github. We have downloaded one of these detectors and stored it in the haarcascades directory. In the next code cell, we demonstrate how to use this detector to find human faces in a sample image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#extract pre trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('../input/images/pp/haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "#load the bGR image\n",
    "bgr_image =cv2.imread(human_images[0])\n",
    "#it will be in form of X*X pixels but if also includes another argument it will be for color channel \n",
    "\n",
    "print(type(bgr_image))\n",
    "print(bgr_image.shape)\n",
    "\n",
    "#convert the image to gray scale , it does not have any color channel so it will in x*y pixels\n",
    "gray_img = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "print(type(gray_img))\n",
    "print(gray_img.shape)\n",
    "      \n",
    "#find faces in GRAY image , it will have four coordinate l, b ,h ,w\n",
    "faces = face_cascade.detectMultiScale(gray_img)\n",
    "print(faces)\n",
    "#The detectMultiScale function executes the classifier stored in face_cascade and takes the grayscale image as a parameter. \n",
    "#gives us the coordinates of a “box” of where the human face is detected\n",
    "\n",
    "#print no. of faces detected in an image\n",
    "print('The no. of faces detected', len(faces))\n",
    "\n",
    "#make a boundary for each detected faces\n",
    "for(x,y,w,h) in faces:\n",
    "    cv2.rectangle(bgr_image, (x,y), (x+w, y+h), (255,150,0), 2)  #2 denotes width of boundary line before it  there is color channel for rgb\n",
    " \n",
    "#convert bgr image  to rgb  for plotting\n",
    "cv_rgb = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB) \n",
    "print(cv_rgb.shape)\n",
    "\n",
    "\n",
    "#display the image along with boundary box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return s true if face detected in image store at imgpath\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0\n",
    "\n",
    "print(face_detector(human_images[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays human_files_short and dog_files_short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#tqdm uses smart algorithms to predict the remaining time and to skip unnecessary iteration displays, which allows for a negligible overhead in most cases.\n",
    "#tqdm your loops show a smart progress meter \n",
    "\n",
    "human_files_short = human_images[:100]\n",
    "dog_files_short = dog_images[:100]\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "\n",
    "def performance_face_detector_opencv(human_files_short, dog_files_short):\n",
    "    \n",
    "    face_of_human = [face_detector(human_img) for human_img in tqdm(human_files_short)]\n",
    "    percentage_human = 100 * (sum(face_of_human)/len(face_of_human))\n",
    "    print(percentage_human, '% of human faces detected in human_images')\n",
    "    \n",
    "    face_of_dog = [face_detector(dog_img) for dog_img in tqdm(dog_files_short)]\n",
    "    percentage_dog = 100 * (sum(face_of_dog)/len(face_of_dog))\n",
    "    print(percentage_dog, '% of human faces detected in dog_images')\n",
    "    \n",
    "performance_face_detector_opencv(human_files_short, dog_files_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Detect Dogs \n",
    "\n",
    "In this section, we use a pre-trained model to detect dogs in images.\n",
    "Obtain Pre-trained VGG-16 Model\n",
    "\n",
    "The code cell below downloads the VGG-16 model, along with weights that have been trained on ImageNet, a very large, very popular dataset used for image classification and other vision tasks. ImageNet contains over 10 million URLs, each linking to an image containing an object from one of 1000 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "#define VGG16 model\n",
    "VGG16 = models.vgg16(pretrained = True)\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    VGG16 = VGG16.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * (IMPLEMENTATION) Making Predictions with a Pre-trained Model\n",
    "\n",
    "In the next code cell, you will write a function that accepts a path to an image (such as 'dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg') as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model. The output should always be an integer between 0 and 999, inclusive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set PIL to be tolerant of image files that are truncated.\n",
    "#The ImageFile module provides support functions for the image open and save functions.\n",
    "from PIL import ImageFile\n",
    "\n",
    "#There seems to be at least one image that is truncated which will cause an exception when it's loaded so this next setting lets us ignore the error and keep working.\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def VGG16_predict(img_path):\n",
    "    \n",
    "    #Here we have loaded the image using the Image library of PIL\n",
    "    #Open jpg \n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "     # convert img to tensor, to give it as an input for VGG16\n",
    "     #we have created object for transforming\n",
    "    image_transform = transforms.Compose([transforms.RandomResizedCrop(250),\n",
    "                                             transforms.ToTensor()])\n",
    "\n",
    "#\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "# Converts a PIL Image or numpy.ndarray (H x W x C) in the range[0, 255]\n",
    "# to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    " \n",
    "\n",
    "    #Here we are transforming the image\n",
    "    img_tensor = image_transform(img)\n",
    "   \n",
    "    # PyTorch pretrained models expect the Tensor dims to be (num input imgs, num color channels, height, width).\n",
    "    # Currently however, we have (num color channels, height, width); let's fix this by inserting a new axis.\n",
    "    img_tensor = img_tensor.unsqueeze(0) # Insert the new axis at index 0 i.e. in front of the other axes/dims.\n",
    "    \n",
    "      # move tensor to cuda\n",
    "    if torch.cuda.is_available():\n",
    "        img_tensor = img_tensor.cuda()\n",
    "\n",
    "    prediction = VGG16(img_tensor)\n",
    "    \n",
    "    # move tensor to cpu, for cpu processing\n",
    "    if torch.cuda.is_available():\n",
    "        prediction = prediction.cpu()\n",
    "\n",
    "       \n",
    "    #to get index \n",
    "    index = prediction.data.numpy().argmax()\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will define the path to be passed\n",
    "image_path = '../input/images/pp/dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'\n",
    "pred_index = VGG16_predict(image_path)\n",
    "print(pred_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write a DOG Detector using function\n",
    "\n",
    "While looking at the dictionary, you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from 'Chihuahua' to 'Mexican hairless'. Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    idexx = VGG16_predict(img_path)\n",
    "    return  ((idexx <= 268) & (idexx >= 151))\n",
    "\n",
    "print(dog_detector(dog_images[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "\n",
    "face_human = [dog_detector(img) for img in  tqdm(human_files_short)]\n",
    "percentage_human = 100 * sum(face_human)/len(face_human)\n",
    "print (percentage_human, '{}% of faces detected  of dog in the first 100 images in human_files by OpenCV')\n",
    "    \n",
    "face_dog = [dog_detector(img) for img in tqdm(dog_files_short)]\n",
    "percentage_dog = 100 * sum(face_dog)/len(face_dog)\n",
    "print(percentage_dog,'% of the first 100 images in dog_files had a dog face detected in them.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images. In this step, you will create a CNN that classifies dog breeds. You must create your CNN from scratch (so, you can't use transfer learning yet!), and you must attain a test accuracy of at least 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "### TODO: Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "\n",
    "transform_pipeline = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.ImageFolder('../input/images/pp/dogImages/train', transform=transform_pipeline)\n",
    "valid_data = datasets.ImageFolder('../input/images/pp/dogImages/valid', transform=transform_pipeline)\n",
    "test_data = datasets.ImageFolder('../input/images/pp/dogImages/test', transform=transform_pipeline)\n",
    "\n",
    "# print out some data stats\n",
    "print('Num training images: ', len(train_data))\n",
    "print('Num test images: ', len(test_data))\n",
    "print('Num validation images: ' , len(valid_data))\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers,\n",
    "                                           shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers,\n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers,\n",
    "                                           shuffle=False)\n",
    "loaders_scratch = {\n",
    "    'train': train_loader,\n",
    "    'valid': valid_loader,\n",
    "    'test': test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "total_dog_classes = 133 # total classes of dog\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.norm2d1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        # pool\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        size_linear_layer = 500\n",
    "        \n",
    "        # linear layer (128 * 28 * 28 -> 500)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, size_linear_layer)\n",
    "        self.fc2 = nn.Linear(size_linear_layer, total_dog_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.norm2d1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # flatten image input\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "   \n",
    "#-#-# You so NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "print(model_scratch)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch = model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path ='../input/images/pp/dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'\n",
    "#img = Image.open(img_path)\n",
    "#img_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    " #                                  transforms.ToTensor()])\n",
    "#img_tensor = img_transform(img)\n",
    "#img_tensor = img_tensor.unsqueeze(0)\n",
    "#print(img_tensor.shape)\n",
    "\n",
    "#model(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a loss function and optimizer. Save the chosen loss function as criterion_scratch, and the optimizer as optimizer_scratch below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer_scratch = optim.SGD(model_scratch.parameters(), lr=0.01)\n",
    "\n",
    "if use_cuda:\n",
    "    criterion_scratch = criterion_scratch.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Train and validate your model in the code cell below. Save the final model parameters at filepath 'model_scratch.pt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                #cuda0 = torch.device('cuda:0')  # CUDA GPU 0\n",
    "                #data = data.to(cuda0)\n",
    "                #target = target.to(cuda0)\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            #if batch_idx % 100 == 0:\n",
    "            #    print('Epoch %d, Batch %d loss: %.6f' % (epoch, batch_idx + 1, train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "                  .format(valid_loss_min, valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_scratch.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *(IMPLEMENTATION) Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images. Your CNN must attain at least 60% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify data loaders\n",
    "loaders_transfer = loaders_scratch.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Use transfer learning to create a CNN to classify dog breed. Use the code cell below, and save your initialized model as the variable model_transfer.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet is chosen as known as excellent performance for image classification.\n",
    "#final fully-connected layer is add with fully-connected layer with output of 133 (total calsses of dog).\n",
    "import torchvision.models as models\n",
    "import torch.nn as  nn\n",
    "\n",
    "## TODO: Specify model architecture \n",
    "model_transfer = models.resnet50(pretrained=True)\n",
    "print(model_transfer)\n",
    "\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_transfer.fc = nn.Linear(2048, 133, bias=True)\n",
    "print(model_transfer.fc)\n",
    "\n",
    "fc_parameters = model_transfer.fc.parameters()\n",
    "\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.SGD(model_transfer.fc.parameters(), lr =0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            #if batch_idx % 100 == 0:\n",
    "            #    print('Epoch %d, Batch %d loss: %.6f' % (epoch, batch_idx + 1, train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "                  .format(valid_loss_min, valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "# train the model\n",
    "model_transfer = train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer,\n",
    "                       use_cuda, 'model_transfer.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * (IMPLEMENTATION) Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "data_transfer = loaders_transfer.copy()\n",
    "print(data_transfer)\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = [item[4:].replace(\"_\", \" \") for item in data_transfer['train'].dataset.classes]\n",
    "print(len(class_names))\n",
    "\n",
    "def predict_breed_classifier(img_path):\n",
    "    global model_transfer\n",
    "    global data_transform\n",
    "    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    image_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
    "                                             transforms.ToTensor()])\n",
    "\n",
    "#\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "# Converts a PIL Image or numpy.ndarray (H x W x C) in the range[0, 255]\n",
    "# to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    " \n",
    "\n",
    "    #Here we are transforming the image\n",
    "    img_tensor = image_transform(image)\n",
    "   \n",
    "    # PyTorch pretrained models expect the Tensor dims to be (num input imgs, num color channels, height, width).\n",
    "    # Currently however, we have (num color channels, height, width); let's fix this by inserting a new axis.\n",
    "    img_tensor = img_tensor.unsqueeze(0) # Insert the new axis at index 0 i.e. in front of the other axes/dims.\n",
    "    \n",
    "      # move tensor to cuda\n",
    "    if torch.cuda.is_available():\n",
    "        img_tensor = img_tensor.cuda()\n",
    "\n",
    "    prediction = model_transfer(img_tensor)\n",
    "    print(type(prediction))\n",
    "    \n",
    "    # move tensor to cpu, for cpu processing\n",
    "    if torch.cuda.is_available():\n",
    "        prediction = prediction.cpu()\n",
    "\n",
    "       \n",
    "    #to get index invpytorch\n",
    "    idx = torch.argmax(prediction)\n",
    "    \n",
    "    return  class_names[idx]\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST PREDICT\n",
    "for img_file in os.listdir('../input/images/pp/dogImages/test/001.Affenpinscher'):\n",
    "    img_path = os.path.join('../input/images/pp/dogImages/test/001.Affenpinscher', img_file)\n",
    "    prediction =  predict_breed_classifier(img_path)\n",
    "    print(\"image_file_name: {0}, \\t predition breed: {1}\".format(img_path, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither. Then,\n",
    "\n",
    "    if a dog is detected in the image, return the predicted breed.\n",
    "    if a human is detected in the image, return the resembling dog breed.\n",
    "    if neither is detected in the image, provide output that indicates an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_app(img_path):\n",
    "    ## handle cases for a human face, dog, and neither\n",
    "    if face_detector(img_path) > 0:\n",
    "        breed = predict_breed_transfer(img_path)\n",
    "        print('Human / resembing dog breed is ' + breed)\n",
    "    elif dog_detector(img_path):\n",
    "        breed = predict_breed_transfer(img_path)\n",
    "        print('Dog / dog breed is ' + breed)       \n",
    "    else:\n",
    "        print('Not Dog, Neither Human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Test Your Algorithm¶\n",
    "\n",
    "In this section, you will take your new algorithm for a spin! What kind of dog does the algorithm think that you look like? If you have a dog, does it predict your dog's breed accurately? If you have a cat, does it mistakenly think that your cat is a dog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "for img_file in os.listdir('../input/images/pp/lfw'):\n",
    "    img_path = os.path.join('../input/images/pp/lfw', img_file)\n",
    "    run_app(img_path)\n",
    "    print(img_path)\n",
    "    img = Image.open(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "for img_file in os.listdir('../input/images/pp/doggImages'):\n",
    "    img_path = os.path.join('../input/images/pp/dogImages', img_file)\n",
    "    run_app(img_path)\n",
    "    print(img_path)\n",
    "    img = Image.open(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
